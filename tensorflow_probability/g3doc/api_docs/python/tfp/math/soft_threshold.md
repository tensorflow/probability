<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tfp.math.soft_threshold" />
<meta itemprop="path" content="Stable" />
</div>

# tfp.math.soft_threshold

Soft Thresholding operator.

``` python
tfp.math.soft_threshold(
    x,
    threshold,
    name=None
)
```



Defined in [`python/math/generic.py`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/math/generic.py).

<!-- Placeholder for "Used in" -->

This operator is defined by the equations

```none
                              { x[i] - gamma,  x[i] >   gamma
SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma
                              { x[i] + gamma,  x[i] <  -gamma
```

In the context of proximal gradient methods, we have

```none
SoftThreshold(x, gamma) = prox_{gamma L1}(x)
```

where `prox` is the proximity operator.  Thus the soft thresholding operator
is used in proximal gradient descent for optimizing a smooth function with
(non-smooth) L1 regularization, as outlined below.

The proximity operator is defined as:

```none
prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },
```

where `r` is a (weakly) convex function, not necessarily differentiable.
Because the L2 norm is strictly convex, the above argmin is unique.

One important application of the proximity operator is as follows.  Let `L` be
a convex and differentiable function with Lipschitz-continuous gradient.  Let
`R` be a convex lower semicontinuous function which is possibly
nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then

```none
x_star = argmin{ L(x) + R(x) : x }
```

if and only if the fixed-point equation is satisfied:

```none
x_star = prox_{gamma R}(x_star - gamma grad L(x_star))
```

Proximal gradient descent thus typically consists of choosing an initial value
`x^{(0)}` and repeatedly applying the update

```none
x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))
```

where `gamma` is allowed to vary from iteration to iteration.  Specializing to
the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly
applying the update

```
x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)
```

(This idea can also be extended to second-order approximations, although the
multivariate case does not have a known closed form like above.)

#### Args:


* <b>`x`</b>: `float` `Tensor` representing the input to the SoftThreshold function.
* <b>`threshold`</b>: nonnegative scalar, `float` `Tensor` representing the radius of
  the interval on which each coordinate of SoftThreshold takes the value
  zero.  Denoted `gamma` above.
* <b>`name`</b>: Python string indicating the name of the TensorFlow operation.
  Default value: `'soft_threshold'`.


#### Returns:


* <b>`softthreshold`</b>: `float` `Tensor` with the same shape and dtype as `x`,
  representing the value of the SoftThreshold function.

#### References

[1]: Yu, Yao-Liang. The Proximity Operator.
     https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf

[2]: Wikipedia Contributors. Proximal gradient methods for learning.
     _Wikipedia, The Free Encyclopedia_, 2018.
     https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning