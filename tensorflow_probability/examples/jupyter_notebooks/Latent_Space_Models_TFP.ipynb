{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latent_Space_Models_TFP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ldRhFo81WscB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "VMOWFkQfWsMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtUBlTmlUCE9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Latent Space Models for Neural Data with TFP\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Latent_Space_Models_TFP.ipynb\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Latent_Space_Models_TFP.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Original content [this Repository](https://github.com/blei-lab/edward/blob/master/notebooks/latent_space_models.ipynb) and [this tutorial](http://edwardlib.org/tutorials/latent-space-models), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/). The initial version of this tutorial was written by Maja Rudolph. [David Ha](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/) also wrote a post on it.\n",
        "\n",
        "Ported to [Tensorflow Probability](https://www.tensorflow.org/probability/) by Matthew McAteer ([`@MatthewMcAteer0`](https://twitter.com/MatthewMcAteer0)), with help from Bryan Seybold, Mike Shwe ([`@mikeshwe`](https://twitter.com/mikeshwe)), Josh Dillon, and the rest of the TFP team at  Google ([`tfprobability@tensorflow.org`](mailto:tfprobability@tensorflow.org)).\n",
        "\n",
        "---\n",
        "\n",
        "- Dependencies & Prerequisites\n",
        "- Introduction\n",
        "- Data\n",
        "- Model\n",
        "- Inference\n",
        "- References"
      ]
    },
    {
      "metadata": {
        "id": "TaIoLupxbekc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies & Prerequisites\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>. \n",
        "    <br>\n",
        "    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following\n",
        "    <br>\n",
        "      <ul>\n",
        "    <li> For the most recent nightly installation: <code>pip3 install -q tfp-nightly</code></li>\n",
        "    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>\n",
        "    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>\n",
        "    <li> For the most recent nightly GPU-connected version of TFP: <code>pip3 install -q tfp-nightly-gpu</code></li>\n",
        "    </ul>\n",
        "Again, if you are running this in a Colab, Tensorflow and TFP are already installed\n",
        "</div>"
      ]
    },
    {
      "metadata": {
        "id": "0zc2vb6EUCFA",
        "colab_type": "code",
        "outputId": "0eb6d164-1c6e-4371-e1a8-25f8d6439d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Imports and Global Variables  { display-mode: \"form\" }\n",
        "!pip3 install -q observations\n",
        "!pip3 install -q wget\n",
        "from __future__ import absolute_import, division, print_function\n",
        "#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
        "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
        "import warnings\n",
        "warnings.filterwarnings(warning_status)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
        "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "from datetime import datetime\n",
        "import os\n",
        "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
        "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
        "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
        "import matplotlib.axes as axes;\n",
        "from matplotlib.patches import Ellipse\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "from IPython.core.pylabtools import figsize\n",
        "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
        "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
        "%config InlineBackend.figure_format = notebook_screen_res\n",
        "\n",
        "import tensorflow as tf\n",
        "tfe = tf.contrib.eager\n",
        "\n",
        "# Eager Execution\n",
        "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
        "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
        "use_tf_eager = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Use try/except so we can easily re-execute the whole notebook.\n",
        "if use_tf_eager:\n",
        "  try:\n",
        "    tf.enable_eager_execution()\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "  \n",
        "def evaluate(tensors):\n",
        "  \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
        "  Args:\n",
        "  tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
        "    `namedtuple` or combinations thereof.\n",
        " \n",
        "  Returns:\n",
        "    ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
        "      `EagerTensor`s replaced by Numpy `ndarray`s.\n",
        "  \"\"\"\n",
        "  if tf.executing_eagerly():\n",
        "    return tf.contrib.framework.nest.pack_sequence_as(\n",
        "        tensors,\n",
        "        [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
        "         for t in tf.contrib.framework.nest.flatten(tensors)])\n",
        "  return sess.run(tensors)\n",
        "\n",
        "class _TFColor(object):\n",
        "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
        "    red = '#F15854'\n",
        "    blue = '#5DA5DA'\n",
        "    orange = '#FAA43A'\n",
        "    green = '#60BD68'\n",
        "    pink = '#F17CB0'\n",
        "    brown = '#B2912F'\n",
        "    purple = '#B276B2'\n",
        "    yellow = '#DECF3F'\n",
        "    gray = '#4D4D4D'\n",
        "    def __getitem__(self, i):\n",
        "        return [\n",
        "            self.red,\n",
        "            self.orange,\n",
        "            self.green,\n",
        "            self.blue,\n",
        "            self.pink,\n",
        "            self.brown,\n",
        "            self.purple,\n",
        "            self.yellow,\n",
        "            self.gray,\n",
        "        ][i % 9]\n",
        "TFColor = _TFColor()\n",
        "\n",
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "    \n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "\n",
        "def reset_sess(config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to create the TF graph & session or reset them.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = session_options()\n",
        "    global sess\n",
        "    tf.reset_default_graph()\n",
        "    try:\n",
        "        sess.close()\n",
        "    except:\n",
        "        pass\n",
        "    sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "reset_sess()\n",
        "\n",
        "\n",
        "# from edward.models import Normal, Poisson\n",
        "from observations import celegans\n",
        "import networkx as nx\n",
        "from observations.util import maybe_download_and_extract\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6sGkB9-DYote",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Many scientific fields involve the study of network data, including social networks, networks in statistical physics, biological networks, and information networks (Goldenberg, Zheng, Fienberg, & Airoldi, 2010; Newman, 2010).\n",
        "\n",
        "What we can learn about nodes in a network from their connectivity patterns? We can begin to study this using a latent space model (Hoff, Raftery, & Handcock, 2002). Latent space models embed nodes in the network in a latent space, where the likelihood of forming an edge between two nodes depends on their distance in the latent space.\n",
        "\n",
        "We will analyze network data from neuroscience."
      ]
    },
    {
      "metadata": {
        "id": "zy8CBHKTUCFH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "The data comes from [Mark Newman's repository](http://www-personal.umich.edu/~mejn/netdata/).\n",
        "It is a weighted, directed network representing the neural network of\n",
        "the nematode\n",
        "[C. Elegans](https://en.wikipedia.org/wiki/Caenorhabditis_elegans)\n",
        "compiled by Watts & Strogatz (1998) using experimental data\n",
        "by White, Southgate, Thomson, & Brenner (1986).\n",
        "\n",
        "The neural network consists of around $300$ neurons. Each connection\n",
        "between neurons\n",
        "is associated with a weight (positive integer) capturing the strength\n",
        "of the connection.\n",
        "\n",
        "First, we load the data."
      ]
    },
    {
      "metadata": {
        "id": "5qlbiQXhoD9h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "url = 'https://raw.githubusercontent.com/matthew-mcateer/external_project_images/master/celegansneural.gml'\n",
        "filename = wget.download(url)\n",
        "filename\n",
        "\n",
        "# Set seed. Remove this line to generate different mixtures!\n",
        "tf.set_random_seed(77)\n",
        "\n",
        "# Our Multigraph containing information on 297 nodes, numbered 1 through 306\n",
        "graph = nx.read_gml('celegansneural.gml')\n",
        "\n",
        "# We create a blank adjacency matrix based on the number of nodes (and accounting for missing nodes)\n",
        "x_train = np.zeros([graph.number_of_nodes() + 9, \n",
        "                    graph.number_of_nodes() + 9],\n",
        "                    dtype=np.int)\n",
        "for i, j in graph.edges():\n",
        "    x_train[int(i)-1, int(j)-1] = int(graph[i][j][0]['value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pPf3Z4Fvt6F-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have a np.darray `x_train`, an ajacency matrix with $306$ rows and $306$ columns."
      ]
    },
    {
      "metadata": {
        "id": "OPDx03R4UCFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "What can we learn about the neurons from their connectivity patterns? Using\n",
        "a latent space model (Hoff et al., 2002), we will learn a latent\n",
        "embedding for each neuron to capture the similarities between them.\n",
        "\n",
        "Each neuron $n$ is a node in the network and is associated with a latent\n",
        "position $z_n\\in\\mathbb{R}^K$.\n",
        "We place a Gaussian prior on each of the latent positions.\n",
        "\n",
        "The log-odds of an edge between node $i$ and\n",
        "$j$ is proportional to the Euclidean distance between the latent\n",
        "representations of the nodes $|z_i- z_j|$. Here, we\n",
        "model the weights ($Y_{ij}$) of the edges with a Poisson likelihood.\n",
        "The rate is the reciprocal of the distance in latent space. The\n",
        "generative process is as follows:\n",
        "\n",
        "1. \n",
        "For each node $n=1,\\ldots,N$,\n",
        "\\begin{align}\n",
        "z_n \\sim \\text{Normal}(0,I).\n",
        "\\end{align}\n",
        "2. \n",
        "For each edge $(i,j)\\in\\{1,\\ldots,N\\}\\times\\{1,\\ldots,N\\}$,\n",
        "\\begin{align}\n",
        "Y_{ij} \\sim \\text{Poisson}\\Bigg(\\frac{1}{|z_i - z_j|}\\Bigg).\n",
        "\\end{align}\n",
        "\n",
        "In TFP, we write the model for the $N \\times N$ distance matrix as follows.\n",
        "\n",
        "1. Create a vector, $[\\parallel{z_1} \\parallel^2, \\parallel{z_2} \\parallel^2, ..., \\parallel{z_N} \\parallel^2]$, and tile it (with `tf.tile`) to create $N$ identical rows.\n",
        "2. Create a $N \\times N$ matrix where entry $(i, j)$ has a value of $||z_i||^2 + ||z_j||^2 - 2 z_i^T z_j$.\n",
        "3. Invert the pairwise distances and make rate along diagonals to be close to zero."
      ]
    },
    {
      "metadata": {
        "id": "LDg5KGCIUCFM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Model Hyperparameters { display-mode: \"form\" }\n",
        "#@markdown number of data points (default is 306, resulting from the shape of the dataset)\n",
        "N = x_train.shape[0]  #@param {type:\"raw\"} \n",
        "#@markdown latent dimensionality (default is 3)\n",
        "K = 3                 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "def trainable_normal(x, layer_fn=tf.layers.dense, loc_fn=lambda x: x, \n",
        "                     scale_fn=1., name=None):\n",
        "    \"\"\"Constructs a trainable `tfd.Normal` distribution.\n",
        "    This function creates a Normal distribution parameterized by loc and scale.\n",
        "    Using default args, this function is mathematically equivalent to:\n",
        "    Args:\n",
        "      x: `Tensor` with floating type. Must have statically defined rank and\n",
        "        statically known right-most dimension.\n",
        "      layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n",
        "        returns a transformation of `x` with shape\n",
        "        `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n",
        "        Default value: `tf.layers.dense`.\n",
        "      loc_fn: Python `callable` which transforms the `loc` parameter. Takes a\n",
        "        (batch of) length-`dims` vectors and returns a `Tensor` of same shape and\n",
        "        `dtype`.\n",
        "        Default value: `lambda x: x`.\n",
        "      scale_fn: Python `callable` or `Tensor`. If a `callable` transforms the\n",
        "        `scale` parameters; if `Tensor` is the `tfd.Normal` `scale` argument.\n",
        "        Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n",
        "        size. (Taking a `callable` or `Tensor` is how `tf.Variable` intializers\n",
        "        behave.)\n",
        "        Default value: `1`.\n",
        "      name: A `name_scope` name for operations created by this function.\n",
        "        Default value: `None` (i.e., \"normal\").\n",
        "    Returns:\n",
        "      normal: An instance of `tfd.Normal`.\n",
        "    \"\"\"\n",
        "    x = tf.convert_to_tensor(x, name='x')\n",
        "    if callable(scale_fn):\n",
        "        y = layer_fn(x, 2)\n",
        "        loc = loc_fn(y[..., 0])\n",
        "        scale = scale_fn(y[..., 1])\n",
        "    else:\n",
        "        y = tf.squeeze(layer_fn(x, 1), axis=-1)\n",
        "        loc = loc_fn(y)\n",
        "        scale = tf.cast(scale_fn, loc.dtype.base_dtype)\n",
        "    return tfd.Normal(loc=loc, scale=scale)\n",
        "\n",
        "\n",
        "def softplus_and_shift(x, shift=1e-5, name=None):\n",
        "    \"\"\"Converts (batch of) scalars to (batch of) positive valued scalars.\n",
        "    Args:\n",
        "      x: (Batch of) `float`-like `Tensor` representing scalars which will be\n",
        "        transformed into positive elements.\n",
        "      shift: `Tensor` added to `softplus` transformation of elements.\n",
        "        Default value: `1e-5`.\n",
        "      name: A `name_scope` name for operations created by this function.\n",
        "        Default value: `None` (i.e., \"positive_tril_with_shift\").\n",
        "    Returns:\n",
        "      scale: (Batch of) scalars`with `x.dtype` and `x.shape`.\n",
        "    \"\"\"\n",
        "    x = tf.convert_to_tensor(x, name='x')\n",
        "    y = tf.nn.softplus(x)\n",
        "    if shift is not None:\n",
        "        y += shift\n",
        "    return y\n",
        "\n",
        "\n",
        "def tril_with_diag_softplus_and_shift(x, diag_shift=1e-5, name=None):\n",
        "    \"\"\"Converts (batch of) vectors to (batch of) lower-triangular scale matrices.\n",
        "    Args:\n",
        "      x: (Batch of) `float`-like `Tensor` representing vectors which will be\n",
        "        transformed into lower-triangular scale matrices with positive diagonal\n",
        "        elements. Rightmost shape `n` must be such that\n",
        "        `n = dims * (dims + 1) / 2` for some positive, integer `dims`.\n",
        "      diag_shift: `Tensor` added to `softplus` transformation of diagonal\n",
        "        elements.\n",
        "        Default value: `1e-5`.\n",
        "      name: A `name_scope` name for operations created by this function.\n",
        "        Default value: `None` (i.e., \"tril_with_diag_softplus_and_shift\").\n",
        "    Returns:\n",
        "      scale_tril: (Batch of) lower-triangular `Tensor` with `x.dtype` and\n",
        "        rightmost shape `[dims, dims]` where `n = dims * (dims + 1) / 2` where\n",
        "        `n = x.shape[-1]`.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, 'tril_with_diag_softplus_and_shift',\n",
        "                       [x, diag_shift]):\n",
        "        x = tf.convert_to_tensor(x, name='x')\n",
        "        x = tfd.fill_triangular(x)\n",
        "        diag = softplus_and_shift(tf.matrix_diag_part(x), diag_shift)\n",
        "        x = tf.matrix_set_diag(x, diag)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "def trainable_multivariate_normal_tril(x, dims, layer_fn=tf.layers.dense,\n",
        "      loc_fn=lambda x: x, scale_fn=tril_with_diag_softplus_and_shift,\n",
        "      name=None):\n",
        "    \"\"\"Constructs a trainable `tfd.MultivariateNormalTriL` distribution.\n",
        "    Args:\n",
        "      x: `Tensor` with floating type. Must have statically defined rank and\n",
        "        statically known right-most dimension.\n",
        "      dims: Scalar, `int`, `Tensor` indicated the MVN event size, i.e., the\n",
        "        created MVN will be distribution over length-`dims` vectors.\n",
        "      layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n",
        "        returns a transformation of `x` with shape\n",
        "        `tf.concat([tf.shape(x)[:-1], [d]], axis=0)`.\n",
        "        Default value: `tf.layers.dense`.\n",
        "      loc_fn: Python `callable` which transforms the `loc` parameter. Takes a\n",
        "        (batch of) length-`dims` vectors and returns a `Tensor` of same shape and\n",
        "        `dtype`.\n",
        "        Default value: `lambda x: x`.\n",
        "      scale_fn: Python `callable` which transforms the `scale` parameters. Takes a\n",
        "        (batch of) length-`dims * (dims + 1) / 2` vectors and returns a\n",
        "        lower-triangular `Tensor` of same batch shape with rightmost dimensions\n",
        "        having shape `[dims, dims]`.\n",
        "        Default value: `tril_with_diag_softplus_and_shift`.\n",
        "      name: A `name_scope` name for operations created by this function.\n",
        "        Default value: `None` (i.e., \"multivariate_normal_tril\").\n",
        "    Returns:\n",
        "      mvntril: An instance of `tfd.MultivariateNormalTriL`.\n",
        "    \"\"\"\n",
        "    x = tf.convert_to_tensor(x, name='x')\n",
        "    x = layer_fn(x, dims + dims * (dims + 1) // 2)\n",
        "    return tfd.MultivariateNormalTriL(\n",
        "        loc=loc_fn(x[..., :dims]),\n",
        "        scale_tril=scale_fn(x[..., dims:]))\n",
        "\n",
        "\n",
        "class Progbar(object):\n",
        "    def __init__(self, target, width=30, interval=0.01, verbose=1):\n",
        "        \"\"\"(Yet another) progress bar.\n",
        "        Args:\n",
        "          target: int.\n",
        "            Total number of steps expected.\n",
        "          width: int.\n",
        "            Width of progress bar.\n",
        "          interval: float.\n",
        "            Minimum time (in seconds) for progress bar to be displayed\n",
        "            during updates.\n",
        "          verbose: int.\n",
        "            Level of verbosity. 0 suppresses output; 1 is default.\n",
        "        \"\"\"\n",
        "        self.target = target\n",
        "        self.width = width\n",
        "        self.interval = interval\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.stored_values = {}\n",
        "        self.start = time.time()\n",
        "        self.last_update = 0\n",
        "        self.total_width = 0\n",
        "        self.seen_so_far = 0\n",
        "\n",
        "    def update(self, current, values=None, force=False):\n",
        "        \"\"\"Update progress bar, and print to standard output if `force`\n",
        "        is True, or the last update was completed longer than `interval`\n",
        "        amount of time ago, or `current` >= `target`.\n",
        "        The written output is the progress bar and all unique values.\n",
        "        Args:\n",
        "          current: int.\n",
        "            Index of current step.\n",
        "          values: dict of str to float.\n",
        "            Dict of name by value-for-last-step. The progress bar\n",
        "            will display averages for these values.\n",
        "          force: bool.\n",
        "            Whether to force visual progress update.\n",
        "        \"\"\"\n",
        "        if values is None:\n",
        "            values = {}\n",
        "\n",
        "        for k, v in six.iteritems(values):\n",
        "            self.stored_values[k] = v\n",
        "\n",
        "        self.seen_so_far = current\n",
        "\n",
        "        now = time.time()\n",
        "        if (not force and\n",
        "                (now - self.last_update) < self.interval and\n",
        "                current < self.target):\n",
        "            return\n",
        "\n",
        "        self.last_update = now\n",
        "        if self.verbose == 0:\n",
        "            return\n",
        "\n",
        "        prev_total_width = self.total_width\n",
        "        sys.stdout.write(\"\\b\" * prev_total_width)\n",
        "        sys.stdout.write(\"\\r\")\n",
        "\n",
        "        # Write progress bar to stdout.\n",
        "        n_digits = len(str(self.target))\n",
        "        bar = '%%%dd/%%%dd' % (n_digits, n_digits) % (current, self.target)\n",
        "        bar += ' [{0}%]'.format(str(int(current / self.target * 100)).rjust(3))\n",
        "        bar += ' '\n",
        "        prog_width = int(self.width * float(current) / self.target)\n",
        "        if prog_width > 0:\n",
        "            try:\n",
        "                bar += ('█' * prog_width)\n",
        "            except UnicodeEncodeError:\n",
        "                bar += ('*' * prog_width)\n",
        "\n",
        "        bar += (' ' * (self.width - prog_width))\n",
        "        sys.stdout.write(bar)\n",
        "\n",
        "        # Write values to stdout.\n",
        "        if current:\n",
        "            time_per_unit = (now - self.start) / current\n",
        "        else:\n",
        "            time_per_unit = 0\n",
        "\n",
        "        eta = time_per_unit * (self.target - current)\n",
        "        info = ''\n",
        "        if current < self.target:\n",
        "            info += ' ETA: %ds' % eta\n",
        "        else:\n",
        "            info += ' Elapsed: %ds' % (now - self.start)\n",
        "\n",
        "        for k, v in six.iteritems(self.stored_values):\n",
        "            info += ' | {0:s}: {1:0.3f}'.format(k, v)\n",
        "\n",
        "        self.total_width = len(bar) + len(info)\n",
        "        if prev_total_width > self.total_width:\n",
        "            info += ((prev_total_width - self.total_width) * \" \")\n",
        "\n",
        "        sys.stdout.write(info)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if current >= self.target:\n",
        "            sys.stdout.write(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1ibNsw90CUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "z = tfd.Normal(loc=tf.zeros([N, K]), scale=tf.ones([N, K]))\n",
        "z_samples_ = evaluate(tfd.Normal(loc=tf.zeros([N]), scale=tf.ones([N])).sample(sample_shape=N))\n",
        "\n",
        "qz = trainable_multivariate_normal_tril(z_samples_, dims=K)\n",
        "\n",
        "# Calculate N x N distance matrix.\n",
        "# 1. Create a vector, [||z_1||^2, ||z_2||^2, ..., ||z_N||^2], and tile it to create N identical rows.\n",
        "xp = tf.tile(tf.reduce_sum(tf.pow(qz.sample(), 2), 1, keepdims=True), [1, N])\n",
        "\n",
        "# 2. Create a N x N matrix where entry (i, j) is ||z_i||^2 + ||z_j||^2 - 2 z_i^T z_j.\n",
        "xp = xp + tf.transpose(xp) - 2 * tf.matmul(qz.sample(), qz.sample(), transpose_b=True)\n",
        "\n",
        "# 3. Invert the pairwise distances and make rate along diagonals to be close to zero.\n",
        "xp = 1.0 / tf.sqrt(xp + tf.diag(tf.zeros(N) + 1e3))\n",
        "\n",
        "xpn = tf.where(tf.is_nan(xp), tf.fill([N, N], 1e3), xp)\n",
        "\n",
        "x = tfd.Poisson(rate=xpn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fJzpKo_nMce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "bec85ce6-ab07-41e3-8f72-b21a1882f291"
      },
      "cell_type": "code",
      "source": [
        "evaluate(tf.global_variables_initializer())\n",
        "evaluate(x.log_prob(x_train))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.1015174e-02, -1.8210919e+00, -1.6628826e-01, ...,\n",
              "        -2.2830762e-01, -1.0000000e+03, -1.0000000e+03],\n",
              "       [-1.6523896e-01, -3.1186100e-02, -4.6782112e-01, ...,\n",
              "        -2.4599613e-01, -2.6316521e-01, -2.3615609e-01],\n",
              "       [-1.5785871e-01, -1.6311169e-01, -3.1892154e-02, ...,\n",
              "        -1.0000000e+03, -1.0000000e+03, -1.0000000e+03],\n",
              "       ...,\n",
              "       [-1.5375145e-01, -1.8544824e-01, -4.4960254e-01, ...,\n",
              "        -3.1586692e-02, -1.0000000e+03, -1.0000000e+03],\n",
              "       [-1.9456385e-01, -2.8933319e-01, -1.0000000e+03, ...,\n",
              "        -2.6937908e-01, -3.1537995e-02, -2.7287695e-01],\n",
              "       [-1.4607909e-01, -1.3248882e-01, -1.0000000e+03, ...,\n",
              "        -1.0000000e+03, -6.4602989e-01, -3.1521451e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "TT2AdLnVUCFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Maximum a posteriori (MAP) estimation is simple in Edward. Two lines are\n",
        "required: Instantiating inference and running it."
      ]
    },
    {
      "metadata": {
        "id": "BwCUhtETUCFQ",
        "colab_type": "code",
        "outputId": "0b4a174a-f44f-4edf-f3c3-e4dac1ac0934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "# Maximum A Posteriori estimation\n",
        "loss = -tf.reduce_sum(x.log_prob(x_train))\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=2.**-4).minimize(loss)\n",
        "\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "# Run graph 2500 times.\n",
        "num_steps = 240\n",
        "loss_ = np.zeros(num_steps)   # Style: `_` to indicate evaluate result.\n",
        "evaluate(init_op)\n",
        "for it in range(loss_.size):\n",
        "    _, loss_[it] = evaluate([train_op, loss])\n",
        "    if it % 20 == 0 or it == loss_.size - 1:\n",
        "        print(\"iterations:{}  loss:{:.6f}\".format(it, loss_[it]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterations:0  loss:7536977.000000\n",
            "iterations:20  loss:93586608.000000\n",
            "iterations:40  loss:93586608.000000\n",
            "iterations:60  loss:93586608.000000\n",
            "iterations:80  loss:93586608.000000\n",
            "iterations:100  loss:93586608.000000\n",
            "iterations:120  loss:93586608.000000\n",
            "iterations:140  loss:93586608.000000\n",
            "iterations:160  loss:93586608.000000\n",
            "iterations:180  loss:93586608.000000\n",
            "iterations:200  loss:93586608.000000\n",
            "iterations:220  loss:93586608.000000\n",
            "iterations:239  loss:93586608.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2iRWNyBxUCFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. Goldenberg, A., Zheng, A. X., Fienberg, S. E., & Airoldi, E. M. (2010). [A survey of statistical network models.](https://www.nowpublishers.com/article/Details/MAL-005) Foundations and Trends in Machine Learning.\n",
        "2. Hoff, P. D., Raftery, A. E., & Handcock, M. S. (2002). [Latent space approaches to social network analysis.](https://amstat.tandfonline.com/doi/abs/10.1198/016214502388618906#.W2X9PN-YW8g) Journal of the American Statistical Association, 97(460), 1090–1098.\n",
        "3. Newman, M. (2010). [Networks: An introduction.](https://books.google.com/books?hl=en&lr=&id=YdZjDwAAQBAJ&oi=fnd&pg=PP1&dq=networks+an+introduction+newman&ots=V_IZ1Qkcrv&sig=e32XDZs8gzBAHzsBGEYVmWQhsis#v=onepage&q=networks%20an%20introduction%20newman&f=false) Oxford University Press.\n",
        "4. Watts, D. J., & Strogatz, S. H. (1998). [Collective dynamics of ‘small-world’networks.](https://www.nature.com/articles/30918) Nature, 393(6684), 440–442.\n",
        "5. White, J. G., Southgate, E., Thomson, J. N., & Brenner, S. (1986). [The structure of the nervous system of the nematode caenorhabditis elegans.](https://pdfs.semanticscholar.org/0bb2/1a76b5604211927cbc3c18f64437adbd834a.pdf) Philos Trans R Soc Lond B Biol Sci, 314(1165), 1–340."
      ]
    },
    {
      "metadata": {
        "id": "GbwzzzGkUCFb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "def css_styling():\n",
        "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
        "    return HTML(styles)\n",
        "css_styling()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}