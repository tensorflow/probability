{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_TFP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bLETbMQ07Z5S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "BYU_QwkC7L10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNKo0BTDRd5t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Long Short-Term Memory Network (LSTM) with TFP\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/LSTM_TFP.ipynb\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/LSTM_TFP.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/lstm.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/).\n",
        "\n",
        "Ported to [Tensorflow Probability](https://www.tensorflow.org/probability/) by Matthew McAteer ([`@MatthewMcAteer0`](https://twitter.com/MatthewMcAteer0)), with help from Bryan Seybold, Mike Shwe ([`@mikeshwe`](https://twitter.com/mikeshwe)), Josh Dillon, and the rest of the TFP team at  Google ([`tfprobability@tensorflow.org`](mailto:tfprobability@tensorflow.org)).\n",
        "\n",
        "---\n",
        "\n",
        "- Dependencies & Prerequisites\n",
        "  - Introduction\n",
        "  - Recurrent Neural Networks\n",
        "  - The Problem of Long-term Dependencies\n",
        "  - LSTM Networks\n",
        "  - The Core Idea Behind LSTMs\n",
        "  - Step-by-Step LSTM Walk Through\n",
        "- A LSTM language model run on text8\n",
        "  - Data\n",
        "  - Model\n",
        "  - Inference\n",
        "  - Variants on Long Short Term Memory\n",
        "  - Conclusions\n",
        "- References\n"
      ]
    },
    {
      "metadata": {
        "id": "_CrDtYMfz9bj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies & Prerequisites\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>. \n",
        "    <br>\n",
        "    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following\n",
        "    <br>\n",
        "      <ul>\n",
        "    <li> For the most recent nightly installation: <code>pip3 install -q tfp-nightly</code></li>\n",
        "    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>\n",
        "    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>\n",
        "    <li> For the most recent nightly GPU-connected version of TFP: <code>pip3 install -q tfp-nightly-gpu</code></li>\n",
        "    </ul>\n",
        "Again, if you are running this in a Colab, Tensorflow and TFP are already installed\n",
        "</div>"
      ]
    },
    {
      "metadata": {
        "id": "ADaJn1yq-bKF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Imports and Global Variables  { display-mode: \"form\" }\n",
        "!pip3 install -q observations\n",
        "from observations import text8\n",
        "!pip3 install -q corner\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
        "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
        "import warnings\n",
        "warnings.filterwarnings(warning_status)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
        "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
        "\n",
        "import functools\n",
        "import six\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import string\n",
        "from datetime import datetime\n",
        "import os\n",
        "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
        "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
        "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
        "import matplotlib.axes as axes;\n",
        "from matplotlib.patches import Ellipse\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "from IPython.core.pylabtools import figsize\n",
        "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
        "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
        "%config InlineBackend.figure_format = notebook_screen_res\n",
        "\n",
        "import tensorflow as tf\n",
        "tfe = tf.contrib.eager\n",
        "\n",
        "# Eager Execution\n",
        "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
        "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
        "use_tf_eager = False #@param {type:\"boolean\"}\n",
        "# Use try/except so we can easily re-execute the whole notebook.\n",
        "if use_tf_eager:\n",
        "    try:\n",
        "        tf.enable_eager_execution()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "  \n",
        "def evaluate(tensors):\n",
        "  \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
        "  Args:\n",
        "  tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
        "    `namedtuple` or combinations thereof.\n",
        " \n",
        "  Returns:\n",
        "    ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
        "      `EagerTensor`s replaced by Numpy `ndarray`s.\n",
        "  \"\"\"\n",
        "  if tf.executing_eagerly():\n",
        "      return tf.contrib.framework.nest.pack_sequence_as(\n",
        "          tensors,\n",
        "          [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
        "           for t in tf.contrib.framework.nest.flatten(tensors)])\n",
        "  return sess.run(tensors)\n",
        "\n",
        "class _TFColor(object):\n",
        "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
        "    red = '#F15854'\n",
        "    blue = '#5DA5DA'\n",
        "    orange = '#FAA43A'\n",
        "    green = '#60BD68'\n",
        "    pink = '#F17CB0'\n",
        "    brown = '#B2912F'\n",
        "    purple = '#B276B2'\n",
        "    yellow = '#DECF3F'\n",
        "    gray = '#4D4D4D'\n",
        "    def __getitem__(self, i):\n",
        "        return [\n",
        "            self.red,\n",
        "            self.orange,\n",
        "            self.green,\n",
        "            self.blue,\n",
        "            self.pink,\n",
        "            self.brown,\n",
        "            self.purple,\n",
        "            self.yellow,\n",
        "            self.gray,\n",
        "        ][i % 9]\n",
        "TFColor = _TFColor()\n",
        "\n",
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "    \n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "\n",
        "def reset_sess(config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to create the TF graph & session or reset them.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = session_options()\n",
        "    global sess\n",
        "    tf.reset_default_graph()\n",
        "    try:\n",
        "        sess.close()\n",
        "    except:\n",
        "        pass\n",
        "    sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "reset_sess()\n",
        "\n",
        "\n",
        "class Progbar(object):\n",
        "    def __init__(self, target, width=30, interval=0.01, verbose=1):\n",
        "        \"\"\"Progress bar for displaying remaining time for given operations.\n",
        "        Args:\n",
        "          target: int.\n",
        "            Total number of steps expected.\n",
        "          width: int.\n",
        "            Width of progress bar.\n",
        "          interval: float.\n",
        "            Minimum time (in seconds) for progress bar to be displayed\n",
        "            during updates.\n",
        "          verbose: int.\n",
        "            Level of verbosity. 0 suppresses output; 1 is default.\n",
        "        \"\"\"\n",
        "        self.target = target\n",
        "        self.width = width\n",
        "        self.interval = interval\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.stored_values = {}\n",
        "        self.start = time.time()\n",
        "        self.last_update = 0\n",
        "        self.total_width = 0\n",
        "        self.seen_so_far = 0\n",
        "\n",
        "    def update(self, current, values=None, force=False):\n",
        "        \"\"\"Update progress bar, and print to standard output if `force`\n",
        "        is True, or the last update was completed longer than `interval`\n",
        "        amount of time ago, or `current` >= `target`.\n",
        "        The written output is the progress bar and all unique values.\n",
        "        Args:\n",
        "          current: int.\n",
        "            Index of current step.\n",
        "          values: dict of str to float.\n",
        "            Dict of name by value-for-last-step. The progress bar\n",
        "            will display averages for these values.\n",
        "          force: bool.\n",
        "            Whether to force visual progress update.\n",
        "        \"\"\"\n",
        "        if values is None:\n",
        "            values = {}\n",
        "\n",
        "        for k, v in six.iteritems(values):\n",
        "            self.stored_values[k] = v\n",
        "\n",
        "        self.seen_so_far = current\n",
        "\n",
        "        now = time.time()\n",
        "        if (not force and\n",
        "                (now - self.last_update) < self.interval and\n",
        "                current < self.target):\n",
        "            return\n",
        "\n",
        "        self.last_update = now\n",
        "        if self.verbose == 0:\n",
        "            return\n",
        "\n",
        "        prev_total_width = self.total_width\n",
        "        sys.stdout.write(\"\\b\" * prev_total_width)\n",
        "        sys.stdout.write(\"\\r\")\n",
        "\n",
        "        # Write progress bar to stdout.\n",
        "        n_digits = len(str(self.target))\n",
        "        bar = '%%%dd/%%%dd' % (n_digits, n_digits) % (current, self.target)\n",
        "        bar += ' [{0}%]'.format(str(int(current / self.target * 100)).rjust(3))\n",
        "        bar += ' '\n",
        "        prog_width = int(self.width * float(current) / self.target)\n",
        "        if prog_width > 0:\n",
        "            try:\n",
        "                bar += ('█' * prog_width)\n",
        "            except UnicodeEncodeError:\n",
        "                bar += ('*' * prog_width)\n",
        "\n",
        "        bar += (' ' * (self.width - prog_width))\n",
        "        sys.stdout.write(bar)\n",
        "\n",
        "        # Write values to stdout.\n",
        "        if current:\n",
        "            time_per_unit = (now - self.start) / current\n",
        "        else:\n",
        "            time_per_unit = 0\n",
        "\n",
        "        eta = time_per_unit * (self.target - current)\n",
        "        info = ''\n",
        "        if current < self.target:\n",
        "            info += ' ETA: %ds' % eta\n",
        "        else:\n",
        "            info += ' Elapsed: %ds' % (now - self.start)\n",
        "\n",
        "        for k, v in six.iteritems(self.stored_values):\n",
        "            info += ' | {0:s}: {1:0.3f}'.format(k, v)\n",
        "\n",
        "        self.total_width = len(bar) + len(info)\n",
        "        if prev_total_width > self.total_width:\n",
        "            info += ((prev_total_width - self.total_width) * \" \")\n",
        "\n",
        "        sys.stdout.write(info)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if current >= self.target:\n",
        "            sys.stdout.write(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cf60KUxeWfjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "A lot of processes involve the use of time-series data. In many cases this will involve the use of an architecture like an Recurent Neural Network (an RNN). The problem is that this can often result in predictions that resemble short-term patterns in the data, but do not resemble patterns that take place over longer periods of time (such as words made of characters or sentences made out of words being coherent). An LSTM solves this by being able to remember recent patterns that have taken place over longer stretches of time-steps."
      ]
    },
    {
      "metadata": {
        "id": "yr_lSrkJxNdB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before getting our data, we need to define the directories where we'll store the data, as well as the hyperparameters for our training (number of training epochs, batch size, number of timesteps, hidden layer size, and learning rate)"
      ]
    },
    {
      "metadata": {
        "id": "T5GP3Iv0Rd50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "#@markdown Set seed (for reproducibility). Remove this line to generate different mixtures!\n",
        "random_seed = 77       #@param {type:\"number\"}\n",
        "tf.set_random_seed(77)\n",
        "\n",
        "#@markdown data directory for training data (Default is '/tmp/data')\n",
        "data_dir = '/tmp/data' #@param {type:\"string\"}\n",
        "#@markdown data directory for inference logs (Default is '/tmp/log')\n",
        "log_dir = '/tmp/log'   #@param {type:\"string\"}\n",
        "#@markdown number of epochs (Default is 200)\n",
        "n_epoch = 200          #@param {type:\"number\"}\n",
        "#@markdown batch size (Default is 128)\n",
        "batch_size = 128       #@param {type:\"number\"}\n",
        "#@markdown hidden size (Default is 512)\n",
        "hidden_size = 512      #@param {type:\"number\"}\n",
        "#@markdown timesteps for LSTM (Default is 64)\n",
        "timesteps = 64         #@param {type:\"number\"}\n",
        "#@markdown learning rate for Gradient Descent (Default is 5e-3)\n",
        "lr = 5e-3              #@param {type:\"number\"}\n",
        "\n",
        "timestamp = datetime.strftime(datetime.utcnow(), \"%Y%m%d_%H%M%S\")\n",
        "hyperparam_str = '_'.join([var + '_' + str(eval(var)).replace('.', '_') for var in ['batch_size', 'hidden_size', 'timesteps', 'lr']])\n",
        "log_dir = os.path.join(log_dir, timestamp + '_' + hyperparam_str)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7CDI-DT7Rd50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A LSTM language model run on text8.\n",
        "\n",
        "We're going to run our LSTM on a dataset called `text8`. `text8` is the first $10^8$ bytes ($\\text{~74 MB} $) of a dataset `fil9`. `fil9`, in turn, is the result oftaking the 1 GB file for `enwik9`, and filtering it to produce a $\\text{715 MB}$ file (`fil9`). Where does `enwiki9` come from?\n",
        "\n",
        "`fil9` resulted from [research by Matt Mahoney's group](http://mattmahoney.net/dc/textdata.html) on the entropy of \"clean\" written English (English in a $27$ character alphabet containing only the letters a-z and nonconsecutive spaces, has been estimated to be between $0.6$ and $1.3$ bits per character). Through this, they found that most of the best compressors will compress Wikipedia text (`enwik9`, $\\text{1 GB}$) to around $\\text{}$ $\\text{715 MB}$. The specific Wikipedia text comes from The test data for the first $10^9$ bytes of the English Wikipedia dump on Mar. 3, 2006 (which is the basis for the [Large Text Compression Benchmark](http://mattmahoney.net/dc/text.html))"
      ]
    },
    {
      "metadata": {
        "id": "zxJXREskRd5-",
        "colab_type": "code",
        "outputId": "5d845b52-2706-4efa-8cdc-3ccd82c2c80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "x_train, _, x_test = text8(data_dir)\n",
        "vocab = string.ascii_lowercase + ' '\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Our encoder is a dictionary with letters/symbols as the keys, a\n",
        "# {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, \n",
        "#  'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, \n",
        "#  'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, \n",
        "#  'z': 25, ' ': 26}\n",
        "encoder = dict(zip(vocab, range(vocab_size)))\n",
        "\n",
        "# Our decoder is the reverse of the above dictionary, with the numbers being the \n",
        "# keys and the letters being the iterms\n",
        "decoder = {v: k for k, v in encoder.items()}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading /tmp/data/text8.zip.part \n",
            ">> [29.9 MB/29.9 MB] 100% @1.9 MB/s,[0s remaining, 15s elapsed]        \n",
            "URL http://mattmahoney.net/dc/text8.zip downloaded to /tmp/data/text8.zip \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/observations/util.py:601: ResourceWarning: unclosed <socket.socket fd=57, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 58398), raddr=('67.195.197.75', 80)>\n",
            "  download_file(url, filepath, hash_true, resume)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MscQgF2SGQhn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll set up our LSTM cell. LSTMs are special kind of RNNs with capability of handling Long-Term dependencies. LSTMs also provide solution to Vanishing/Exploding Gradient problem. \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/LSTM_8.PNG?raw=trueg\" width=\"800\">\n",
        "\n",
        "*LSTM cells connected*\n",
        "\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/LSTM_3.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "*LSTM cell internal visual representation*\n",
        "\n",
        "**$f$: *Forget gate***, whether to erase the cell\n",
        "\n",
        "**$i$: *input gate*** , whether to write to the cell\n",
        "\n",
        "**$g$: *Gate gate***, How much to write to the cell\n",
        "\n",
        "**$o$: *Output gate***, How much to reveal the cell\n",
        "\n",
        "Let’s discuss the gates:\n",
        "\n",
        "**Forget Gate:** After getting the output of **previous state, $h(t-1)$**, Forget gate helps us to take decisions about what must be removed from $h(t-1)$ state and thus keeping only relevant stuff. It is surrounded by a sigmoid function which helps to crush the input between $[0,1]$. It is represented as:\n",
        "$$\n",
        "f_t = \\sigma(W_f \t\\cdot [h_{t-1}, x_t]+b_f)\n",
        "$$\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/LSTM_4.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "*Forget Gate*\n",
        "\n",
        "We multiply forget gate with previous cell state to forget the unnecessary stuff from previous state which is not needed anymore, as shown below:\n",
        "\n",
        "\n",
        "**Input Gate:** In the input gate, we decide to add new stuff from the present input to our present cell state scaled by how much we wish to add them.\n",
        "$$\n",
        "i_t = \\sigma(W_i \t\\cdot [h_{t-1}, x_t]+b_i) \\\\\n",
        "\\tilde{C}_t = \\tanh(W_C \t\\cdot [h_{t-1}, x_t]+b_C)\n",
        "$$\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/LSTM_5.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "*Input Gate+Gate_gate*\n",
        "\n",
        "In the above photo, *sigmoid layer decides which values to be updated* and *tanh layer creates a vector for new candidates to added to present cell state*.\n",
        "\n",
        "**Gate Gate:** To calculate the present cell state, we add the output of ( `(input_gate*gate_gate)` and `forget_gate`) as follows:\n",
        "$$\n",
        "C_t = f_t \t\\ast C_{t-1} + i_t \t\\ast \\tilde{C}_t\n",
        "$$\n",
        "\n",
        "**Output Gate:** Finally we’ll decide what to output from our cell state which will be done by our sigmoid function.\n",
        "\n",
        "We multiply the input with tanh to crush the values between $(-1,1)$ and then multiply it with the output of sigmoid function so that we only output what we want to.\n",
        "$$\n",
        "o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o) \\\\\n",
        "h_t = o_t \\ast \\tanh(C_t)\n",
        "$$\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/LSTM_6.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "*Output Gate*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "s8jYyasvRd52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_cell(x, h, c, name=None, reuse=False):\n",
        "    \"\"\"\n",
        "    LSTM returning hidden state and content cell at a specific timestep.\n",
        "    \"\"\"\n",
        "    nin = x.shape[-1].value\n",
        "    nout = h.shape[-1].value\n",
        "    with tf.variable_scope(name, default_name=\"lstm\",\n",
        "                         values=[x, h, c], reuse=reuse):\n",
        "        wx = tf.get_variable(\"kernel/input\", [nin, nout * 4],\n",
        "                             dtype=tf.float32,\n",
        "                             initializer=tf.orthogonal_initializer(1.0))\n",
        "        wh = tf.get_variable(\"kernel/hidden\", [nout, nout * 4],\n",
        "                             dtype=tf.float32,\n",
        "                             initializer=tf.orthogonal_initializer(1.0))\n",
        "        b = tf.get_variable(\"bias\", [nout * 4],\n",
        "                            dtype=tf.float32,\n",
        "                            initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "    z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n",
        "    i, f, o, u = tf.split(z, 4, axis=1)\n",
        "    i = tf.sigmoid(i)\n",
        "    f = tf.sigmoid(f + 1.0)\n",
        "    o = tf.sigmoid(o)\n",
        "    u = tf.tanh(u)\n",
        "    c = f * c + i * u\n",
        "    h = o * tf.tanh(c)\n",
        "    return h, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PItMYcuRWnNa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, this is far more complex than a typical RNN cell. How does this all solve the vanishing gradient cell?\n",
        "\n",
        "1. There is no multiplication with matrix $W$ during backprop. This works by element-wise multiplication with the forget gate ($f$). This has an added bonus of reducing the time complexity\n",
        "\n",
        "2. During backpropapagtion through each LSTM cell, it’s multiplied by different values of forget gate, which makes it less prone to vanishing/exploding gradient.\n",
        "\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/LSTM_9.PNG?raw=true\" width=\"800\">"
      ]
    },
    {
      "metadata": {
        "id": "J6RYcn8KwvG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up our data iterator\n",
        "\n",
        "For our iterator, we're going to use the [tf.Data](https://www.tensorflow.org/guide/datasets) API to frame our `x_train` and `x_test`, as well as construct our iterator\n"
      ]
    },
    {
      "metadata": {
        "id": "d7_SzBFjK-H0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator_fn(input, batch_size, timesteps, encoder):\n",
        "    \"\"\"\n",
        "    Generate batch with respect to input (a list). Encode its\n",
        "    strings to integers, returning an array of shape [batch_size, timesteps].\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        # imb creates `batch_size` random indexes along the length of the input\n",
        "        imb = np.random.randint(0, len(input) - timesteps, batch_size)\n",
        "        # `encoded` is a size (`batch_size`, `timesteps`) array of the character \n",
        "        # encodings\n",
        "        encoded = np.asarray(\n",
        "            [[encoder[c] for c in input[i:(i + timesteps)]] for i in imb],\n",
        "            dtype=np.int32)\n",
        "    yield encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yyqjhbqewt5_",
        "colab_type": "code",
        "outputId": "aa45c1c6-2fce-44b7-c4ed-d50a962123ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "# Sets up the training dataset\n",
        "train_dataset = tf.data.Dataset().batch(batch_size).from_generator(\n",
        "    functools.partial(generator_fn, x_train, batch_size, timesteps, encoder),\n",
        "    output_types= tf.int64,\n",
        "    output_shapes=(tf.TensorShape([batch_size, timesteps])))\n",
        "\n",
        "# Sets up the iterator for the training data, which only looks at a small section\n",
        "# at a time\n",
        "train_iterator = train_dataset.make_initializable_iterator()\n",
        "\n",
        "# `x_ph` represents the `.get_next()` for the iteratr\n",
        "x_ph = train_iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nd5uCALK4Pyh",
        "colab_type": "code",
        "outputId": "17282498-1064-41d0-96c7-a3f2ee2acd53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "imb = range(0, len(x_test) - timesteps, timesteps)\n",
        "encoded_x_test = np.asarray(\n",
        "      [[encoder[c] for c in x_test[i:(i + timesteps)]] for i in imb],\n",
        "      dtype=np.int32)\n",
        "\n",
        "test_size = encoded_x_test.shape[0]\n",
        "print(\"Test set shape: {}\".format(encoded_x_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set shape: (78124, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hi3NplAeza5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### TFP in LSTMs\n",
        "\n",
        "Within the Language model, we're going to feed the logits to a Categorical distirbution from TFP.\n",
        "\n",
        "Form $p(x_{0}, ..., x_{\\text{timesteps} - 1})$,\n",
        "$$\n",
        "    \\prod_{t=0}^\\text{timesteps - 1} p(x_t | x_{>t}),\n",
        "$$\n",
        "To calculate the probability, we call `log_prob` on\n",
        "\n",
        "$x = \\{x_{0}, ..., x_{\\text{timesteps} - 1}\\}$ given $\\text{input} = \\{0, x_{0}, ..., x_{\\text{timesteps} - 2}\\}$.\n",
        "\n",
        "We implement this separately from the generative model so the forward pass, e.g., embedding/dense layers, can be parallelized. `[batch_size, timesteps] -> [batch_size, timesteps]`\n"
      ]
    },
    {
      "metadata": {
        "id": "8XiIvWpDRd56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def language_model(input, vocab_size):\n",
        "    \"\"\"\n",
        "    Our Language model for processing the logits\n",
        "    \n",
        "    Args:\n",
        "      input: scalar of true price estimate, taken from state\n",
        "      vocab_size: scalar of prize 1 estimate, to be added to the  prize 1 \n",
        "    Returns: \n",
        "      Categorical distribution of the \n",
        "    Closure over: data_mu, data_std, mu_prior, std_prior\n",
        "    \"\"\"\n",
        "    x = tf.one_hot(input, depth=vocab_size, dtype=tf.float32)\n",
        "    h = tf.fill(tf.stack([tf.shape(x)[0], hidden_size]), 0.0)\n",
        "    c = tf.fill(tf.stack([tf.shape(x)[0], hidden_size]), 0.0)\n",
        "    hs = []\n",
        "    reuse = None\n",
        "    for t in range(timesteps):\n",
        "        if t > 0:\n",
        "            reuse = True\n",
        "        xt = x[:, t, :]\n",
        "        h, c = lstm_cell(xt, h, c, name=\"lstm\", reuse=reuse)\n",
        "        hs.append(h)\n",
        "\n",
        "    h = tf.stack(hs, 1)\n",
        "    logits = tf.layers.dense(h, vocab_size, name=\"dense\")\n",
        "    output = tfd.Categorical(logits=logits)\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8goTTWkNvYKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then define the generator for the language model, which can be summarized with the following relationship:\n",
        "$$\n",
        "x ~ \\prod p(x_t | x_{<t})\n",
        "$$\n",
        "From this we get an output of the `batch_size` and the `vocab_size`"
      ]
    },
    {
      "metadata": {
        "id": "DA2-8S61Rd57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def language_model_gen(batch_size, vocab_size):\n",
        "    \"\"\"\n",
        "    Generates x ~ prod p(x_t | x_{<t}). Output [batch_size, timesteps].\n",
        "    \"\"\"\n",
        "    # Initialize data input randomly.\n",
        "    x = tf.random_uniform([batch_size], 0, vocab_size, dtype=tf.int32)\n",
        "    h = tf.zeros([batch_size, hidden_size])\n",
        "    c = tf.zeros([batch_size, hidden_size])\n",
        "    xs = []\n",
        "    for _ in range(timesteps):\n",
        "        x = tf.one_hot(x, depth=vocab_size, dtype=tf.float32)\n",
        "        h, c = lstm_cell(x, h, c, name=\"lstm\")\n",
        "        logits = tf.layers.dense(h, vocab_size, name=\"dense\")\n",
        "        x = tfd.Categorical(logits=logits).sample()  \n",
        "        xs.append(x)\n",
        "\n",
        "    xs = tf.cast(tf.stack(xs, 1), tf.int32)\n",
        "    return xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9JwKjfTRd6D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model and Inference\n",
        "\n",
        "For our optimization, we will use gradient descent to minimize our loss metric, `test_nll`, a stand in for \"**n**egative **l**og **l**oss\""
      ]
    },
    {
      "metadata": {
        "id": "INv4g17uQkGO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_ph = next_train_batch\n",
        "\n",
        "with tf.variable_scope(\"language_model\"):\n",
        "    # Shift input sequence to right by 1, [0, x[0], ..., x[timesteps - 2]].\n",
        "    x_ph_shift = tf.pad(x_ph, [[0, 0], [1, 0]])[:, :-1]\n",
        "    x = language_model(x_ph_shift, vocab_size)\n",
        "\n",
        "with tf.variable_scope(\"language_model\", reuse=True):\n",
        "    x_gen = language_model_gen(5, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LnZd6X7s9FUY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The TEST Negative Log-likelihood will be used when the test data \n",
        "# is assigned to x_ph\n",
        "test_nll = -tf.reduce_sum(x.log_prob(x_ph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BuB7pLRQg0m",
        "colab_type": "code",
        "outputId": "24656d69-7db7-4116-ac9f-f8d7ab01a0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# The TRAIN Negative Log-likelihood will be used when the test data \n",
        "# is assigned to x_ph\n",
        "train_nll = -tf.reduce_sum(x.log_prob(x_ph))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "train_op = optimizer.minimize(train_nll)\n",
        "\n",
        "print(\"Number of sets of parameters: {}\".format(\n",
        "      len(tf.trainable_variables())))\n",
        "print(\"Number of parameters: {}\".format(\n",
        "      np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])))\n",
        "for v in tf.trainable_variables():\n",
        "    print(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sets of parameters: 5\n",
            "Number of parameters: 1119771\n",
            "<tf.Variable 'language_model/lstm/kernel/input:0' shape=(27, 2048) dtype=float32_ref>\n",
            "<tf.Variable 'language_model/lstm/kernel/hidden:0' shape=(512, 2048) dtype=float32_ref>\n",
            "<tf.Variable 'language_model/lstm/bias:0' shape=(2048,) dtype=float32_ref>\n",
            "<tf.Variable 'language_model/dense/kernel:0' shape=(512, 27) dtype=float32_ref>\n",
            "<tf.Variable 'language_model/dense/bias:0' shape=(27,) dtype=float32_ref>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "drYkfIuq1NQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's initialize our model training. "
      ]
    },
    {
      "metadata": {
        "id": "ePvhCD0J1LTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "evaluate(init_op)\n",
        "evaluate(train_iterator.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwuMCJF-Rd6E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Double n_epoch and print progress every half an epoch.\n",
        "n_iter_per_epoch = len(x_train) // (batch_size * timesteps * 2)\n",
        "epoch = 0.0\n",
        "for _ in range(n_epoch * 2):\n",
        "    epoch += 0.5\n",
        "    print(\"Epoch: {0}\".format(epoch))\n",
        "    avg_nll = 0.0\n",
        "\n",
        "    pbar = Progbar(n_iter_per_epoch)\n",
        "    for t in range(1, n_iter_per_epoch + 1):\n",
        "        pbar.update(t)\n",
        "        #evaluate(assign_train_op)\n",
        "        [_, train_nll_] = evaluate([train_op, train_nll])\n",
        "        avg_nll += train_nll_\n",
        "\n",
        "    # Print average bits per character over epoch.\n",
        "    avg_nll /= (n_iter_per_epoch * batch_size * timesteps *\n",
        "                np.log(2))\n",
        "    print(\"Train average bits/char: {:0.8f}\".format(avg_nll))\n",
        "\n",
        "    ## Print per-data point log-likelihood on test set.\n",
        "    #avg_nll = 0.0\n",
        "    #for start in range(0, test_size, batch_size):\n",
        "    #    end = min(test_size, start + batch_size)\n",
        "    #    x_batch = encoded_x_test[start:end]\n",
        "    #    x_ph = x_batch\n",
        "    #    avg_nll += evaluate(test_nll)\n",
        "\n",
        "    #avg_nll /= test_size\n",
        "    #print(\"Test average NLL: {:0.8f}\".format(avg_nll))\n",
        "\n",
        "    # Generate samples from model.\n",
        "    samples = evaluate(x_gen)\n",
        "    samples = [''.join([decoder[c] for c in sample]) for sample in samples]\n",
        "    print(\"Samples:\")\n",
        "    for sample in samples:\n",
        "        print(sample)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-087dAhYbyG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The default hyperparameters above achieve a negative log-likelihood of about $\\text{~} 78.4$  by epoch $50$, and a negative log-likelihood at around $\\text{~}76.1423$ at epoch $200$;\n",
        "\n",
        "If you're impatient and just instantly want to know what that gets you, after $200$ epochs, we should be getting samples like the following:\n",
        "```\n",
        "e the classmaker was cut apart rome the charts sometimes known a\n",
        "hemical place baining examples of equipment accepted manner clas\n",
        "uetean meeting sought to exist as this waiting an excerpt for of\n",
        "erally enjoyed a film writer of unto one two volunteer humphrey\n",
        "y captured by the saughton river goodness where stones were nota\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "20VCfdbBRd6G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[1] https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "[2] https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "[3] http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\n",
        "\n",
        "[4] http://www.bioinf.jku.at/publications/older/2604.pdf\n",
        "\n",
        "[5] ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf\n",
        "\n",
        "[6] https://arxiv.org/pdf/1406.1078v3.pdf\n",
        "\n",
        "[7] https://arxiv.org/pdf/1508.03790v2.pdf\n",
        "\n",
        "[8] https://arxiv.org/pdf/1402.3511v1.pdf\n",
        "\n",
        "[9] https://arxiv.org/pdf/1503.04069.pdf\n",
        "\n",
        "[10] http://proceedings.mlr.press/v37/jozefowicz15.pdf\n",
        "\n",
        "[11] https://arxiv.org/pdf/1502.03044v2.pdf\n",
        "\n",
        "[12] https://arxiv.org/pdf/1507.01526v1.pdf\n",
        "\n",
        "[13] https://arxiv.org/pdf/1502.04623.pdf\n",
        "\n",
        "[14] https://arxiv.org/pdf/1502.04623.pdf\n",
        "\n",
        "[15] https://arxiv.org/pdf/1411.7610v3.pdf"
      ]
    }
  ]
}