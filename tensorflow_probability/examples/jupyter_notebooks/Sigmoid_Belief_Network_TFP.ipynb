{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sigmoid_Belief_Network_TFP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AxDfcPsLtXVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "Cs7i1ZEUtXsy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPFoRUIPRTaD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sigmoid Belief Network with TFP\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Sigmoid_Belief_Network_TFP.ipynb\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Sigmoid_Belief_Network_TFP.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/sigmoid_belief_network.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)\n",
        "\n",
        "Ported to [Tensorflow Probability](https://www.tensorflow.org/probability/) by Matthew McAteer ([`@MatthewMcAteer0`](https://twitter.com/MatthewMcAteer0)), with help from Bryan Seybold, Mike Shwe ([`@mikeshwe`](https://twitter.com/mikeshwe)), Josh Dillon, and the rest of the TFP team at  Google ([`tfprobability@tensorflow.org`](mailto:tfprobability@tensorflow.org)).\n",
        "\n",
        "---\n",
        "\n",
        "- Dependencies & Prerequisites\n",
        "- Introduction\n",
        "  - Data\n",
        "  - Model\n",
        "  - Inference\n",
        "- References"
      ]
    },
    {
      "metadata": {
        "id": "jEui6GrPbbdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies & Prerequisites\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>. \n",
        "    <br>\n",
        "    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following\n",
        "    <br>\n",
        "      <ul>\n",
        "    <li> For the most recent nightly installation: <code>pip3 install -q tfp-nightly</code></li>\n",
        "    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>\n",
        "    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>\n",
        "    <li> For the most recent nightly GPU-connected version of TFP: <code>pip3 install -q tfp-nightly-gpu</code></li>\n",
        "    </ul>\n",
        "Again, if you are running this in a Colab, Tensorflow and TFP are already installed\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MelbdC_ktc_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Imports and Global Variables  { display-mode: \"form\" }\n",
        "!pip3 install -q observations\n",
        "!pip install -q imageio\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
        "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
        "import warnings\n",
        "warnings.filterwarnings(warning_status)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
        "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
        "\n",
        "import six\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import string\n",
        "from datetime import datetime\n",
        "import os\n",
        "import imageio\n",
        "# from edward.models import Bernoulli\n",
        "from observations import caltech101_silhouettes\n",
        "from imageio import imwrite as imsave\n",
        "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
        "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
        "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
        "import matplotlib.axes as axes;\n",
        "from matplotlib.patches import Ellipse\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "from IPython.core.pylabtools import figsize\n",
        "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
        "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
        "%config InlineBackend.figure_format = notebook_screen_res\n",
        "\n",
        "import tensorflow as tf\n",
        "tfe = tf.contrib.eager\n",
        "\n",
        "# Eager Execution\n",
        "\n",
        "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
        "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
        "use_tf_eager = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Use try/except so we can easily re-execute the whole notebook.\n",
        "if use_tf_eager:\n",
        "    try:\n",
        "        tf.enable_eager_execution()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "  \n",
        "def evaluate(tensors):\n",
        "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
        "    Args:\n",
        "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
        "      `namedtuple` or combinations thereof.\n",
        "\n",
        "    Returns:\n",
        "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
        "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
        "    \"\"\"\n",
        "    if tf.executing_eagerly():\n",
        "        return tf.contrib.framework.nest.pack_sequence_as(\n",
        "            tensors,\n",
        "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
        "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
        "    return sess.run(tensors)\n",
        "\n",
        "class _TFColor(object):\n",
        "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
        "    red = '#F15854'\n",
        "    blue = '#5DA5DA'\n",
        "    orange = '#FAA43A'\n",
        "    green = '#60BD68'\n",
        "    pink = '#F17CB0'\n",
        "    brown = '#B2912F'\n",
        "    purple = '#B276B2'\n",
        "    yellow = '#DECF3F'\n",
        "    gray = '#4D4D4D'\n",
        "    def __getitem__(self, i):\n",
        "        return [\n",
        "            self.red,\n",
        "            self.orange,\n",
        "            self.green,\n",
        "            self.blue,\n",
        "            self.pink,\n",
        "            self.brown,\n",
        "            self.purple,\n",
        "            self.yellow,\n",
        "            self.gray,\n",
        "        ][i % 9]\n",
        "TFColor = _TFColor()\n",
        "\n",
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "    \n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "\n",
        "def reset_sess(config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to create the TF graph & session or reset them.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = session_options()\n",
        "    global sess\n",
        "    tf.reset_default_graph()\n",
        "    try:\n",
        "        sess.close()\n",
        "    except:\n",
        "        pass\n",
        "    sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "\n",
        "\n",
        "reset_sess()\n",
        "\n",
        "class Progbar(object):\n",
        "    def __init__(self, target, width=30, interval=0.01, verbose=1):\n",
        "        \"\"\"(Yet another) progress bar.\n",
        "        Args:\n",
        "          target: int.\n",
        "            Total number of steps expected.\n",
        "          width: int.\n",
        "            Width of progress bar.\n",
        "          interval: float.\n",
        "            Minimum time (in seconds) for progress bar to be displayed\n",
        "            during updates.\n",
        "          verbose: int.\n",
        "            Level of verbosity. 0 suppresses output; 1 is default.\n",
        "        \"\"\"\n",
        "        self.target = target\n",
        "        self.width = width\n",
        "        self.interval = interval\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.stored_values = {}\n",
        "        self.start = time.time()\n",
        "        self.last_update = 0\n",
        "        self.total_width = 0\n",
        "        self.seen_so_far = 0\n",
        "\n",
        "    def update(self, current, values=None, force=False):\n",
        "        \"\"\"Update progress bar, and print to standard output if `force`\n",
        "        is True, or the last update was completed longer than `interval`\n",
        "        amount of time ago, or `current` >= `target`.\n",
        "        The written output is the progress bar and all unique values.\n",
        "        Args:\n",
        "          current: int.\n",
        "            Index of current step.\n",
        "          values: dict of str to float.\n",
        "            Dict of name by value-for-last-step. The progress bar\n",
        "            will display averages for these values.\n",
        "          force: bool.\n",
        "            Whether to force visual progress update.\n",
        "        \"\"\"\n",
        "        if values is None:\n",
        "            values = {}\n",
        "\n",
        "        for k, v in six.iteritems(values):\n",
        "            self.stored_values[k] = v\n",
        "\n",
        "        self.seen_so_far = current\n",
        "\n",
        "        now = time.time()\n",
        "        if (not force and\n",
        "                (now - self.last_update) < self.interval and\n",
        "                current < self.target):\n",
        "            return\n",
        "\n",
        "        self.last_update = now\n",
        "        if self.verbose == 0:\n",
        "            return\n",
        "\n",
        "        prev_total_width = self.total_width\n",
        "        sys.stdout.write(\"\\b\" * prev_total_width)\n",
        "        sys.stdout.write(\"\\r\")\n",
        "\n",
        "        # Write progress bar to stdout.\n",
        "        n_digits = len(str(self.target))\n",
        "        bar = '%%%dd/%%%dd' % (n_digits, n_digits) % (current, self.target)\n",
        "        bar += ' [{0}%]'.format(str(int(current / self.target * 100)).rjust(3))\n",
        "        bar += ' '\n",
        "        prog_width = int(self.width * float(current) / self.target)\n",
        "        if prog_width > 0:\n",
        "            try:\n",
        "                bar += ('█' * prog_width)\n",
        "            except UnicodeEncodeError:\n",
        "                bar += ('*' * prog_width)\n",
        "\n",
        "        bar += (' ' * (self.width - prog_width))\n",
        "        sys.stdout.write(bar)\n",
        "\n",
        "        # Write values to stdout.\n",
        "        if current:\n",
        "            time_per_unit = (now - self.start) / current\n",
        "        else:\n",
        "            time_per_unit = 0\n",
        "\n",
        "        eta = time_per_unit * (self.target - current)\n",
        "        info = ''\n",
        "        if current < self.target:\n",
        "            info += ' ETA: %ds' % eta\n",
        "        else:\n",
        "            info += ' Elapsed: %ds' % (now - self.start)\n",
        "\n",
        "        for k, v in six.iteritems(self.stored_values):\n",
        "            info += ' | {0:s}: {1:0.3f}'.format(k, v)\n",
        "\n",
        "        self.total_width = len(bar) + len(info)\n",
        "        if prev_total_width > self.total_width:\n",
        "            info += ((prev_total_width - self.total_width) * \" \")\n",
        "\n",
        "        sys.stdout.write(info)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if current >= self.target:\n",
        "            sys.stdout.write(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IidYCPxqRTaG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "A Sigmoid belief network [[1]](#scrollTo=2rGFv5Y2RTap) is a type of Belief Network. This is a class of neural network, composed of an acyclic graph of stochastic variables (These have a state of 1 or 0, and the probability of turning on is determined by the weighted input from other units plus a bias). Some of these variables are latent (\"hidden\"), and some are visible to us. \n",
        "\n",
        "If we're using a belief network, we'd lke to solve two specific problems: \n",
        "- **Inference problem:** Infer the states of the unobserved variables.\n",
        "- **Learning problem:** Adjust the interactions between variables to make the network more likely to generate the observed data.\n",
        "\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/Belief_net.PNG?raw=true\" width=400>\n",
        "\n",
        "## The general rules behind a Sigmoid Belief Network\n",
        "\n",
        "There are some pros and cons to the Belief network:\n",
        "\n",
        "**Pros:** It is easy to generate an unbiased example at the leaf nodes, so we can see what kinds of data the network believes in.\n",
        "\n",
        "**Cons:** It is hard to infer the posterior distribution over all possible configurations of hidden causes.  It is hard to even get a\n",
        "sample from the posterior.\n",
        "\n",
        "Looking at those cons, one would wonder if it's possible to scale this belief net architecture up to account for millions of parameters (like many other neural networks architectures)?\n",
        "\n",
        "<img src=\"https://github.com/matthew-mcateer/external_project_images/blob/master/Belief_Net_learning.PNG?raw=true\" width=400>\n",
        "\n",
        "With this in mind, our strategy for updating these weights is the following:\n",
        "$$\n",
        "p_i \\equiv p(s_i = 1) = \\frac{1}{1+\\exp(-\\sum_j s_j w_{ji})} \\\\\n",
        "\\Delta w_{ji} = \\epsilon \\text{ }s_j(s_i-p_i)\n",
        "$$\n",
        "\n",
        "\n",
        "## Our Example\n",
        "\n",
        "For our example, we're going to train our sigmoid belief network on the [Caltech 101 Silhouettes](http://www.cs.toronto.edu/~kswersky/data/) data set. First, let's define our model."
      ]
    },
    {
      "metadata": {
        "id": "TrGOjFE_RTaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "data_dir = \"/tmp/data\"\n",
        "out_dir = \"/tmp/out\"\n",
        "#@markdown Batch size during training\n",
        "batch_size = 24                   #@param\n",
        "#@markdown Hidden size per layer from bottom-up\n",
        "hidden_sizes = [300, 100, 50, 10] #@param\n",
        "#@markdown Number of samples for training\n",
        "n_train_samples = 10              #@param\n",
        "#@markdown Number of samples to calculate test log-lik\n",
        "n_test_samples = 1000             #@param\n",
        "#@markdown Learning rate step size\n",
        "step_size = 1e-3                  #@param\n",
        "n_epoch = 100                     #@param\n",
        "n_iter_per_epoch = 10000          #@param\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyvbD5BzRTaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(array, batch_size):\n",
        "    \"\"\"\n",
        "    Generate batch with respect to array's first axis.\n",
        "    \"\"\"\n",
        "    start = 0  # pointer to where we are in iteration\n",
        "    while True:\n",
        "        stop = start + batch_size\n",
        "        diff = stop - array.shape[0]\n",
        "        if diff <= 0:\n",
        "            batch = array[start:stop]\n",
        "            start += batch_size\n",
        "        else:\n",
        "            batch = np.concatenate((array[start:], array[:diff]))\n",
        "            start = diff\n",
        "        yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCsJoEnsRTaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The [Caltech 101 silhouettes](http://www.cs.toronto.edu/~kswersky/data/) dataset is a dataset of silhouettes for training classifiers. It is a series of binarized (black-and-white) images that serve as the silhouettes of objects to be classified. It was originally created from the [Caltech 101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) dataset (originally used for object recognition), for research on inductive principles for restricted Boltzmann machine learning.\n",
        "\n",
        "<img src=\"https://people.cs.umass.edu/~marlin/data/caltech101s.png\" align=\"middle\">\n",
        "\n",
        "*A sample of images from the Caltech 101 silhouettes*"
      ]
    },
    {
      "metadata": {
        "id": "7uQNrShaRTaY",
        "colab_type": "code",
        "outputId": "104966ce-3fc8-4a6a-93b4-e81ce76de12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "# Set seed. Remove this line to generate different mixtures!\n",
        "tf.set_random_seed(77)\n",
        "\n",
        "(x_train, _), (x_test, _), (x_valid, _) = caltech101_silhouettes(data_dir)\n",
        "x_train_generator = generator(x_train, batch_size)\n",
        "\n",
        "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
        "    x_ph = tf.get_variable(name='x_ph', \n",
        "                           dtype=tf.int32,\n",
        "                           initializer=tf.to_int32(next(x_train_generator)),\n",
        "                           trainable=False\n",
        "                          )\n",
        "\n",
        "# When training, assigns value of x_ph to next batch value\n",
        "assign_train_op = tf.assign(x_ph, value=tf.to_int32(next(x_train_generator)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading /tmp/data/caltech101_silhouettes_28_split1.mat.part \n",
            ">> [851.7 KB/851.7 KB] 120% @2.4 MB/s,[0s remaining, 0s elapsed]        \n",
            "URL http://people.cs.umass.edu/~marlin/data/caltech101_silhouettes_28_split1.mat downloaded to /tmp/data/caltech101_silhouettes_28_split1.mat \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/observations/util.py:601: ResourceWarning: unclosed <socket.socket fd=57, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 40262), raddr=('128.119.240.99', 80)>\n",
            "  download_file(url, filepath, hash_true, resume)\n",
            "/usr/local/lib/python3.6/dist-packages/observations/util.py:601: ResourceWarning: unclosed <ssl.SSLSocket fd=58, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 58202), raddr=('128.119.240.99', 443)>\n",
            "  download_file(url, filepath, hash_true, resume)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CkglAhNSRTac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "We set up the directed graph described below, with the number and sizes of hidden layers that we described earlier."
      ]
    },
    {
      "metadata": {
        "id": "q3UlmxDrRTae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zs = [0] * len(hidden_sizes)\n",
        "for l in reversed(range(len(hidden_sizes))):\n",
        "    if l == len(hidden_sizes) - 1:\n",
        "          logits = tf.zeros([tf.shape(x_ph)[0], hidden_sizes[l]])\n",
        "    else:\n",
        "          logits = tf.layers.dense(tf.cast(zs[l + 1].sample(), tf.float32),\n",
        "                                   hidden_sizes[l],\n",
        "                                   activation=None)\n",
        "    zs[l] = tfd.Bernoulli(logits=logits)\n",
        "\n",
        "x = tfd.Bernoulli(logits=tf.layers.dense(tf.cast(zs[0].sample(), tf.float32),\n",
        "                                         28 * 28, activation=None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFOvbezRKNEg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define the variational model with reverse ordering as probability model. \n",
        "For example: if the layers of $p$ are $15-100-300$ from top-down, then the layers of $q$ are $300-100-15$ from bottom-up."
      ]
    },
    {
      "metadata": {
        "id": "jlPg9ckVK_fR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "qzs = [0] * len(hidden_sizes)\n",
        "for l in range(len(hidden_sizes)):\n",
        "    if l == 0:\n",
        "          logits = tf.layers.dense(tf.cast(x_ph, tf.float32),\n",
        "                                   hidden_sizes[l], activation=None)\n",
        "    else:\n",
        "          logits = tf.layers.dense(tf.cast(qzs[l - 1].sample(), tf.float32),\n",
        "                                   hidden_sizes[l], activation=None)\n",
        "    qzs[l] = tfd.Bernoulli(logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2o0Ynak2RTai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "We can train our model using a combination of log-likelihoods for our model of the test and training data, as well as an Adam Optimizer for minimization of the negative log-loss. Looking back at our diagram for the sigmoid belief network architecture, the inference architecture is just the inverse of that."
      ]
    },
    {
      "metadata": {
        "id": "oUBo_FNERTai",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_sum(x.log_prob(x_ph))\n",
        "for z, qz in zip(zs, qzs):\n",
        "    loss += tf.reduce_mean(z.log_prob(qz.sample(sample_shape=n_train_samples)))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=step_size)\n",
        "train_op = optimizer.minimize(-loss)\n",
        "\n",
        "init_op = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfBebElUVfNx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then train the model below. On the default settings, this takes ~273s / epoch. By epoch 100, we can get these results:\n",
        "- Training negative log-likelihood: `209.443`"
      ]
    },
    {
      "metadata": {
        "id": "4etuen9vRTam",
        "colab_type": "code",
        "outputId": "7f778b66-f07a-467e-d30f-ab76a5764248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate(init_op)\n",
        "for epoch in range(n_epoch):\n",
        "    print(\"Epoch {}\".format(epoch))\n",
        "    train_loss = 0.0\n",
        "     \n",
        "    # Our Progress bar\n",
        "    pbar = Progbar(n_iter_per_epoch)\n",
        "    for t in range(1, n_iter_per_epoch + 1):\n",
        "        pbar.update(t)\n",
        "        evaluate(assign_train_op)\n",
        "        [_, loss_] = evaluate([train_op, loss])\n",
        "        train_loss += loss_\n",
        "\n",
        "    # Print per-data point loss, averaged over training epoch.\n",
        "    train_loss /= n_iter_per_epoch\n",
        "    train_loss /= batch_size\n",
        "    print(\"Training negative log-likelihood: {:0.3f}\".format(-train_loss))\n",
        "\n",
        "    # Prior predictive check.\n",
        "    images = evaluate(x.sample())\n",
        "    for m in range(batch_size):\n",
        "        imsave(\"{}/{}.png\".format(out_dir, m), images[m].astype('uint8').reshape(28, 28))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 344.790\n",
            "Epoch 1\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 342.218\n",
            "Epoch 2\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 341.156\n",
            "Epoch 3\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 339.965\n",
            "Epoch 4\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 337.577\n",
            "Epoch 5\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 336.907\n",
            "Epoch 6\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 335.074\n",
            "Epoch 7\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 334.069\n",
            "Epoch 8\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 335.647\n",
            "Epoch 9\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 333.163\n",
            "Epoch 10\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 332.497\n",
            "Epoch 11\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 331.720\n",
            "Epoch 12\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 329.626\n",
            "Epoch 13\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 330.455\n",
            "Epoch 14\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 328.952\n",
            "Epoch 15\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 327.979\n",
            "Epoch 16\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 326.307\n",
            "Epoch 17\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 324.930\n",
            "Epoch 18\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 322.547\n",
            "Epoch 19\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 321.805\n",
            "Epoch 20\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 318.426\n",
            "Epoch 21\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 316.498\n",
            "Epoch 22\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 314.656\n",
            "Epoch 23\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 313.017\n",
            "Epoch 24\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 312.665\n",
            "Epoch 25\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 310.414\n",
            "Epoch 26\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 308.626\n",
            "Epoch 27\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 306.853\n",
            "Epoch 28\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 305.003\n",
            "Epoch 29\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 304.533\n",
            "Epoch 30\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 303.804\n",
            "Epoch 31\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 302.632\n",
            "Epoch 32\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 297.482\n",
            "Epoch 33\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 294.764\n",
            "Epoch 34\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 291.709\n",
            "Epoch 35\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 292.106\n",
            "Epoch 36\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 286.830\n",
            "Epoch 37\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 282.712\n",
            "Epoch 38\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 280.051\n",
            "Epoch 39\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 277.384\n",
            "Epoch 40\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 273.800\n",
            "Epoch 41\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 269.604\n",
            "Epoch 42\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 268.479\n",
            "Epoch 43\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 266.131\n",
            "Epoch 44\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 265.345\n",
            "Epoch 45\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 264.723\n",
            "Epoch 46\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 263.671\n",
            "Epoch 47\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 262.843\n",
            "Epoch 48\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 261.815\n",
            "Epoch 49\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 259.723\n",
            "Epoch 50\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 257.300\n",
            "Epoch 51\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 255.848\n",
            "Epoch 52\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 255.408\n",
            "Epoch 53\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 253.452\n",
            "Epoch 54\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 251.373\n",
            "Epoch 55\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 249.304\n",
            "Epoch 56\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 246.412\n",
            "Epoch 57\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 243.734\n",
            "Epoch 58\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 240.751\n",
            "Epoch 59\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 235.750\n",
            "Epoch 60\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 233.479\n",
            "Epoch 61\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 232.615\n",
            "Epoch 62\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 232.125\n",
            "Epoch 63\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 231.716\n",
            "Epoch 64\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 231.387\n",
            "Epoch 65\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 230.950\n",
            "Epoch 66\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 229.458\n",
            "Epoch 67\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 228.827\n",
            "Epoch 68\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 227.850\n",
            "Epoch 69\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 227.188\n",
            "Epoch 70\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 226.362\n",
            "Epoch 71\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 225.396\n",
            "Epoch 72\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 224.683\n",
            "Epoch 73\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 223.721\n",
            "Epoch 74\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 223.134\n",
            "Epoch 75\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 223.987\n",
            "Epoch 76\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 223.129\n",
            "Epoch 77\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 222.903\n",
            "Epoch 78\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 221.875\n",
            "Epoch 79\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 221.383\n",
            "Epoch 80\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 221.241\n",
            "Epoch 81\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 220.906\n",
            "Epoch 82\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 219.696\n",
            "Epoch 83\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 218.684\n",
            "Epoch 84\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 217.698\n",
            "Epoch 85\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 217.265\n",
            "Epoch 86\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 216.299\n",
            "Epoch 87\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 216.010\n",
            "Epoch 88\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 215.661\n",
            "Epoch 89\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 214.956\n",
            "Epoch 90\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 214.309\n",
            "Epoch 91\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 213.659\n",
            "Epoch 92\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 213.730\n",
            "Epoch 93\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 213.380\n",
            "Epoch 94\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 273s\n",
            "Training negative log-likelihood: 212.008\n",
            "Epoch 95\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 210.656\n",
            "Epoch 96\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 271s\n",
            "Training negative log-likelihood: 210.359\n",
            "Epoch 97\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 210.258\n",
            "Epoch 98\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 210.018\n",
            "Epoch 99\n",
            "10000/10000 [100%] ██████████████████████████████ Elapsed: 272s\n",
            "Training negative log-likelihood: 209.272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MgoYxykBwxLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "We've seen how a Sigmoid Belief network operates. It should be noted that Sigmoid Belief Networks are subsets of a different architecture: a Deep Belief Network.\n",
        "\n",
        "A DBN is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.[[2]](#scrollTo=2rGFv5Y2RTap). Unlike with a typical Sigmoid Belief network, a DBN can feature bidirectional connections between some of the hidden layers.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Deep_belief_net.svg/440px-Deep_belief_net.svg.png\" width=200>\n",
        "\n",
        "When trained on a set of examples without supervision, a Sigmoid Belief can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors.[[2]](#scrollTo=2rGFv5Y2RTap) After this learning step, a Sigmoid Belief network can be further trained with supervision to perform classification.[[3]](#scrollTo=2rGFv5Y2RTap)\n"
      ]
    },
    {
      "metadata": {
        "id": "2rGFv5Y2RTap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "[1] [Sigmoid belief network (Neal, 1990)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.1777&rep=rep1&type=pdf)\n",
        "\n",
        "[2] Hinton, Geoffrey E. [\"Deep belief networks.\"](http://www.scholarpedia.org/article/Deep_belief_networks) Scholarpedia 4.5 (2009): 5947.\n",
        "\n",
        "[3] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. [\"A fast learning algorithm for deep belief nets.\"](http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) Neural computation 18.7 (2006): 1527-1554."
      ]
    }
  ]
}