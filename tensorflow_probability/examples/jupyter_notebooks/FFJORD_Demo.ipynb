{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ffjord_moons_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzmOdHFzXwPn",
        "colab_type": "text"
      },
      "source": [
        "First install packages used in this demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osWMEZ53VSJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tfp-nightly tf-nightly \"dm-sonnet>=2.0.0b0\" --pre"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC2H2qjyVuS-",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Imports (tf, tfp with adjoint trick, etc)\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tqdm as tqdm\n",
        "import sklearn.datasets as skd\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import kde\n",
        "\n",
        "# tf and friends\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp\n",
        "import sonnet as snt\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "tfb = tfp.bijectors\n",
        "tfd = tfp.distributions\n",
        "\n",
        "def make_grid(xmin, xmax, ymin, ymax, gridlines, pts):\n",
        "  xpts = np.linspace(xmin, xmax, pts)\n",
        "  ypts = np.linspace(ymin, ymax, pts)\n",
        "  xgrid = np.linspace(xmin, xmax, gridlines)\n",
        "  ygrid = np.linspace(ymin, ymax, gridlines)\n",
        "  xlines = np.stack([a.ravel() for a in np.meshgrid(xpts, ygrid)])\n",
        "  ylines = np.stack([a.ravel() for a in np.meshgrid(xgrid, ypts)])\n",
        "  return np.concatenate([xlines, ylines], 1).T\n",
        "\n",
        "grid = make_grid(-3, 3, -3, 3, 4, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S7iVERbVNCF",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions for visualization\n",
        "def plot_density(data, axis):\n",
        "  x, y = np.squeeze(np.split(data, 2, axis=1))\n",
        "  levels = np.linspace(0.0, 0.75, 10)\n",
        "  kwargs = {'levels': levels}\n",
        "  return sns.kdeplot(x, y, cmap=\"viridis\", shade=True, \n",
        "                     shade_lowest=True, ax=axis, **kwargs)\n",
        "\n",
        "\n",
        "def plot_points(data, axis, s=10, color='b', label=''):\n",
        "  x, y = np.squeeze(np.split(data, 2, axis=1))\n",
        "  axis.scatter(x, y, c=color, s=s, label=label)\n",
        "\n",
        "\n",
        "def plot_panel(\n",
        "    grid, samples, transformed_grid, transformed_samples,\n",
        "    dataset, axarray, limits=True):\n",
        "  if len(axarray) != 4:\n",
        "    raise ValueError('Expected 4 axes for the panel')\n",
        "  ax1, ax2, ax3, ax4 = axarray\n",
        "  plot_points(data=grid, axis=ax1, s=20, color='black', label='grid')\n",
        "  plot_points(samples, ax1, s=30, color='blue', label='samples')\n",
        "  plot_points(transformed_grid, ax2, s=20, color='black', label='ode(grid)')\n",
        "  plot_points(transformed_samples, ax2, s=30, color='blue', label='ode(samples)')\n",
        "  ax3 = plot_density(transformed_samples, ax3)\n",
        "  ax4 = plot_density(dataset, ax4)\n",
        "  if limits:\n",
        "    set_limits([ax1], -3.0, 3.0, -3.0, 3.0)\n",
        "    set_limits([ax2], -2.0, 3.0, -2.0, 3.0)\n",
        "    set_limits([ax3, ax4], -1.5, 2.5, -0.75, 1.25)\n",
        "\n",
        "\n",
        "def set_limits(axes, min_x, max_x, min_y, max_y):\n",
        "  if isinstance(axes, list):\n",
        "    for axis in axes:\n",
        "      set_limits(axis, min_x, max_x, min_y, max_y)\n",
        "  else:\n",
        "    axes.set_xlim(min_x, max_x)\n",
        "    axes.set_ylim(min_y, max_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DELBFg0WsLC",
        "colab_type": "text"
      },
      "source": [
        "# FFJORD bijector\n",
        "In this colab we demonstrate FFJORD bijector, originally proposed in the paper by Grathwohl, Will, et al. [arxiv link](https://arxiv.org/pdf/1810.01367.pdf).\n",
        "\n",
        "In the nutshell the idea behind such approach is to establish a correspondence between a known **base distribution** and the **data distribution**.\n",
        "\n",
        "To establish this connection, we need to\n",
        "\n",
        "  1. Define a bijective map $\\mathcal{T}_{\\theta}:\\mathbf{x} \\rightarrow \\mathbf{y}$, $\\mathcal{T}_{\\theta}^{1}:\\mathbf{y} \\rightarrow \\mathbf{x}$ between the space $\\mathcal{Y}$ on which **base distribution** is defined and space $\\mathcal{X}$ of the data domain.\n",
        "  2. Efficiently keep track of the deformations we perform to transfer the notion of probability onto $\\mathcal{X}$.\n",
        "\n",
        "The second condition is formalized in the following expression for probability\n",
        "distribution defined on $\\mathcal{X}$:\n",
        "\n",
        "$$\n",
        "\\log p_{\\mathbf{x}}(\\mathbf{x})=\\log p_{\\mathbf{y}}(\\mathbf{y})-\\log \\operatorname{det}\\left|\\frac{\\partial \\mathcal{T}_{\\theta}(\\mathbf{y})}{\\partial \\mathbf{y}}\\right|\n",
        "$$\n",
        "\n",
        "FFJORD bijector accomplishes this by defining a transformation \n",
        "$$\n",
        "\\mathcal{T_{\\theta}}: \\mathbf{x} = \\mathbf{z}(t_{0}) \\rightarrow \\mathbf{y} = \\mathbf{z}(t_{1}) \\quad : \\quad \\frac{d \\mathbf{z}}{dt} = \\mathbf{f}(t, \\mathbf{z}, \\theta)\n",
        "$$\n",
        "\n",
        "This transformation is invertible, as long as function $\\mathbf{f}$ describing the evolution of the state $\\mathbf{z}$ is well behaved and the `log_det_jacobian` can be calculated by integrating the following expression.\n",
        "\n",
        "$$\n",
        "\\log \\operatorname{det}\\left|\\frac{\\partial \\mathcal{T}_{\\theta}(\\mathbf{y})}{\\partial \\mathbf{y}}\\right| = \n",
        "-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial \\mathbf{f}(t, \\mathbf{z}, \\theta)}{\\partial \\mathbf{z}(t)}\\right) d t\n",
        "$$\n",
        "\n",
        "In this demo we will train a FFJORD bijector to warp a gaussian distribution onto the distribution defined by `moons` dataset. This will be done in 3 steps:\n",
        "  * Define **base distribution**\n",
        "  * Define FFJORD bijector\n",
        "  * Minimize exact log-likelihood of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls2u6WGNWvYB",
        "colab_type": "text"
      },
      "source": [
        "First, we load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWk3YD4zWx_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Dataset\n",
        "DATASET_SIZE = 1024 * 8  #@param\n",
        "BATCH_SIZE = 256  #@param\n",
        "SAMPLE_SIZE = DATASET_SIZE\n",
        "\n",
        "moons = skd.make_moons(n_samples=DATASET_SIZE, noise=.06)[0]\n",
        "\n",
        "moons_ds = tf.data.Dataset.from_tensor_slices(moons.astype(np.float32))\n",
        "moons_ds = moons_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "moons_ds = moons_ds.cache()\n",
        "moons_ds = moons_ds.shuffle(DATASET_SIZE)\n",
        "moons_ds = moons_ds.batch(BATCH_SIZE)\n",
        "\n",
        "plt.figure(figsize=[8, 8])\n",
        "plt.scatter(moons[:, 0], moons[:, 1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsmkbfCdWyFK",
        "colab_type": "text"
      },
      "source": [
        "Next, we instantiate a base distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knDCafOQWyKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_loc = np.array([0.0, 0.0]).astype(np.float32)\n",
        "base_sigma = np.array([0.8, 0.8]).astype(np.float32)\n",
        "base_distribution = tfd.MultivariateNormalDiag(base_loc, base_sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0IOnGoWyNp",
        "colab_type": "text"
      },
      "source": [
        "We use a multi-layer perceptron to model `state_derivative_fn`.\n",
        "\n",
        "While not necessary for this dataset, it is often benefitial to make `state_derivative_fn` dependent on time. Here we achieve this by concatenating `t` to inputs of our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq6mHVh8WyQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP_ODE(snt.Module):\n",
        "  \"\"\"Multi-layer NN ode_fn.\"\"\"\n",
        "  def __init__(self, num_hidden, num_layers, num_output, name='mlp_ode'):\n",
        "    super(MLP_ODE, self).__init__(name=name)\n",
        "    self._num_hidden = num_hidden\n",
        "    self._num_output = num_output\n",
        "    self._num_layers = num_layers\n",
        "    self._modules = []\n",
        "    for _ in range(self._num_layers - 1):\n",
        "      self._modules.append(snt.Linear(self._num_hidden))\n",
        "      self._modules.append(tf.math.tanh)\n",
        "    self._modules.append(snt.Linear(self._num_output))\n",
        "    self._model = snt.Sequential(self._modules)\n",
        "\n",
        "  def __call__(self, t, inputs):\n",
        "    inputs = tf.concat([tf.broadcast_to(t, inputs.shape), inputs], -1)\n",
        "    return self._model(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXeBR0bTWyS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Model and training parameters\n",
        "LR = 1e-2  #@param\n",
        "NUM_EPOCHS = 80  #@param\n",
        "STACKED_FFJORDS = 4  #@param\n",
        "NUM_HIDDEN = 8  #@param\n",
        "NUM_LAYERS = 3  #@param\n",
        "NUM_OUTPUT = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWvaiMDMWyVZ",
        "colab_type": "text"
      },
      "source": [
        "Now we construct a stack of FFJORD bijectors. Each bijector is provided with `ode_solve_fn` and `trace_augmentation_fn` and it's own `state_derivative_fn` model, so that they represent a sequence of different transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPgF4uu-XNM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Building bijector\n",
        "solver = tfp.math.ode.DormandPrince(atol=1e-5)\n",
        "ode_solve_fn = solver.solve\n",
        "trace_augmentation_fn = tfb.ffjord.trace_jacobian_exact\n",
        "\n",
        "bijectors = []\n",
        "for _ in range(STACKED_FFJORDS):\n",
        "  mlp_model = MLP_ODE(NUM_HIDDEN, NUM_LAYERS, NUM_OUTPUT)\n",
        "  next_ffjord = tfb.FFJORD(\n",
        "      state_time_derivative_fn=mlp_model,ode_solve_fn=ode_solve_fn,\n",
        "      trace_augmentation_fn=trace_augmentation_fn)\n",
        "  bijectors.append(next_ffjord)\n",
        "\n",
        "stacked_ffjord = tfb.Chain(bijectors[::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPYgppYXNRk",
        "colab_type": "text"
      },
      "source": [
        "Now we can use `TransformedDistribution` which is the result of warping `base_distribution` with `stacked_ffjord` bijector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5aUlnDrXNU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_distribution = tfd.TransformedDistribution(\n",
        "    distribution=base_distribution, bijector=stacked_ffjord)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpXxHaKJXNYL",
        "colab_type": "text"
      },
      "source": [
        "Now we define our training procedure. We simply minimize negative log-likelihood of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8YC4qpTXXj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@titel Training\n",
        "@tf.function\n",
        "def train_step(optimizer, target_sample):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = -tf.reduce_mean(transformed_distribution.log_prob(target_sample))\n",
        "  variables = tape.watched_variables()\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply(gradients, variables)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ1J7NYxXXnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@titel Samples\n",
        "@tf.function\n",
        "def get_samples():\n",
        "  base_distribution_samples = base_distribution.sample(SAMPLE_SIZE)\n",
        "  transformed_samples = transformed_distribution.sample(SAMPLE_SIZE)\n",
        "  return base_distribution_samples, transformed_samples\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def get_transformed_grid():\n",
        "  transformed_grid = stacked_ffjord.forward(grid)\n",
        "  return transformed_grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbupg1AIXXrb",
        "colab_type": "text"
      },
      "source": [
        "Plot samples from base and transformed distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48XDLdQEXNbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_samples = []\n",
        "base_samples, transformed_samples = get_samples()\n",
        "transformed_grid = get_transformed_grid()\n",
        "evaluation_samples.append((base_samples, transformed_samples, transformed_grid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQyH94U1Xh_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "panel_id = 0\n",
        "panel_data = evaluation_samples[panel_id]\n",
        "fig, axarray = plt.subplots(\n",
        "  1, 4, figsize=(16, 6))\n",
        "plot_panel(\n",
        "    grid, panel_data[0], panel_data[2], panel_data[1], moons, axarray, False)\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo9QhkVpXiGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = tf.Variable(LR, trainable=False)\n",
        "optimizer = snt.optimizers.Adam(learning_rate)\n",
        "\n",
        "for epoch in tqdm.trange(NUM_EPOCHS // 2):\n",
        "  base_samples, transformed_samples = get_samples()\n",
        "  transformed_grid = get_transformed_grid()\n",
        "  evaluation_samples.append(\n",
        "      (base_samples, transformed_samples, transformed_grid))\n",
        "  for batch in moons_ds:\n",
        "    _ = train_step(optimizer, batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2sVjn77XiJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "panel_id = -1\n",
        "panel_data = evaluation_samples[panel_id]\n",
        "fig, axarray = plt.subplots(\n",
        "  1, 4, figsize=(16, 6))\n",
        "plot_panel(grid, panel_data[0], panel_data[2], panel_data[1], moons, axarray)\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaXzmMliXiQ8",
        "colab_type": "text"
      },
      "source": [
        "Training it for longer with learning rate results in further improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIRkQ-33XsSH",
        "colab_type": "text"
      },
      "source": [
        "Not convered in this example, FFJORD bijector supports hutchinson's stochastic trace estimation. The particular estimator can be provided via `trace_augmentation_fn`. Similarly alternative integrators can be used by defining custom `ode_solve_fn`. "
      ]
    }
  ]
}